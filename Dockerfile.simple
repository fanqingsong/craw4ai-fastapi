# Dockerfile for crawl4ai-fastapi
# Using multi-stage build with pip for simplicity

################################
# PYTHON-BASE
# Sets up all our shared environment variables
################################
FROM swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/python:3.11-slim AS python-base

    # python
ENV PYTHONUNBUFFERED=1 \
    # prevents python creating .pyc files
    PYTHONDONTWRITEBYTECODE=1 \
    \
    # pip
    PIP_DISABLE_PIP_VERSION_CHECK=on \
    PIP_DEFAULT_TIMEOUT=100 \
    \
    # paths
    PYSETUP_PATH="/opt/pysetup" \
    VENV_PATH="/opt/pysetup/.venv"

# prepend venv to path
ENV PATH="$VENV_PATH/bin:$PATH"

# Update the package list and install necessary libraries
# 使用阿里云镜像源加速apt下载
RUN echo "deb https://mirrors.aliyun.com/debian/ bookworm main" > /etc/apt/sources.list.d/aliyun.list \
    && echo "deb https://mirrors.aliyun.com/debian-security/ bookworm-security main" >> /etc/apt/sources.list.d/aliyun.list \
    && echo "deb https://mirrors.aliyun.com/debian/ bookworm-updates main" >> /etc/apt/sources.list.d/aliyun.list \
    && apt-get update && apt-get install -y \
    libglib2.0-0 \
    libnss3 \
    libnspr4 \
    libdbus-1-3 \
    libatk1.0-0 \
    libcups2 \
    libcairo2 \
    libpango-1.0-0 \
    libexpat1 \
    libdrm2 \
    libxcb1 \
    libxkbcommon0 \
    libatspi2.0-0 \
    libx11-6 \
    libxcomposite1 \
    libxdamage1 \
    libxrandr2 \
    libxfixes3 \
    libgbm1 \
    libasound2 \
    libatk-bridge2.0-0 \
    && rm -rf /var/lib/apt/lists/*


################################
# BUILDER-BASE
# Used to build deps + create our virtual environment
################################
FROM python-base AS builder-base
# 使用阿里云镜像源加速apt下载
RUN echo "deb https://mirrors.aliyun.com/debian/ bookworm main" > /etc/apt/sources.list.d/aliyun.list \
    && echo "deb https://mirrors.aliyun.com/debian-security/ bookworm-security main" >> /etc/apt/sources.list.d/aliyun.list \
    && echo "deb https://mirrors.aliyun.com/debian/ bookworm-updates main" >> /etc/apt/sources.list.d/aliyun.list \
    && apt-get update \
    && apt-get install --no-install-recommends -y \
        # deps for building python deps
        build-essential

# copy project requirement files here to ensure they will be cached.
WORKDIR $PYSETUP_PATH

# COPY . ./
COPY requirements.txt ./

# 配置pip使用国内镜像源并安装依赖
RUN --mount=type=cache,target=/root/.cache \
    pip config set global.index-url https://mirrors.aliyun.com/pypi/simple/ && \
    pip config set global.trusted-host mirrors.aliyun.com && \
    pip install -r requirements.txt

################################
# DEVELOPMENT
# Image used during development / testing
################################
FROM python-base AS development
ENV FASTAPI_ENV=development
WORKDIR $PYSETUP_PATH

# copy in our built packages
COPY --from=builder-base $PYSETUP_PATH $PYSETUP_PATH

# will become mountpoint of our code
WORKDIR /app
# RUN playwright install

COPY . .

EXPOSE 8000
ENV PROCESSOR_NAME=crawler

CMD ["sh", "-c", "playwright install && uvicorn crawl4ai_fastapi.main:app --host 0.0.0.0 --port 8000 --reload"]


################################
# PRODUCTION
# Final image used for runtime
################################
FROM python-base AS production
ENV FASTAPI_ENV=production
COPY --from=builder-base $PYSETUP_PATH $PYSETUP_PATH

WORKDIR /app
# RUN playwright install

COPY crawl4ai_fastapi ./crawl4ai_fastapi

CMD ["sh", "-c", "playwright install && gunicorn -w 4 -k uvicorn.workers.UvicornWorker -b 0.0.0.0:8000 crawl4ai_fastapi.main:app"]
